# RLM Orchestrator Configuration

max_iterations = 20
max_sub_calls = 50
output_limit = 10000

# Smart bypass: skip RLM for small contexts and send directly to LLM
# This avoids RLM overhead when the full context fits easily
bypass_enabled = true
bypass_threshold = 4000  # chars (~1000 tokens) - contexts smaller than this use direct LLM

# DeepSeek API for root LLM (production - requires DEEPSEEK_API_KEY)
[[providers]]
provider_type = "deepseek"
base_url = "https://api.deepseek.com"
model = "deepseek-chat"
role = "root"
weight = 1

# LiteLLM Proxy for root LLM (alternative - for usage tracking via LiteLLM dashboard)
# Requires LITELLM_API_KEY or LITELLM_MASTER_KEY env var
# [[providers]]
# provider_type = "litellm"
# base_url = "http://localhost:4000"
# model = "deepseek-chat"  # or any model configured in LiteLLM
# role = "root"
# weight = 1

# Local Ollama for root LLM (uncomment for testing without API key)
# [[providers]]
# provider_type = "ollama"
# base_url = "http://localhost:11434"
# model = "qwen2.5-coder:14b"
# role = "root"
# weight = 1

# Local Ollama for sub-LM calls (testing 9B model)
[[providers]]
provider_type = "ollama"
base_url = "http://localhost:11434"
model = "gemma2:9b"
role = "sub"
weight = 1

# Remote Ollama on big72 for sub-LM calls
[[providers]]
provider_type = "ollama"
base_url = "http://big72.local:11434"
model = "mistral:7b"
role = "sub"
weight = 1
