# RLM Orchestrator Configuration

max_iterations = 20
max_sub_calls = 50
output_limit = 10000

# DeepSeek API for root LLM (smarter, follows structured output better)
[[providers]]
provider_type = "deepseek"
base_url = "https://api.deepseek.com"
model = "deepseek-chat"
# API key from environment: DEEPSEEK_API_KEY
role = "root"
weight = 1

# Local Ollama for sub-LM calls (cheaper, faster for simple queries)
[[providers]]
provider_type = "ollama"
base_url = "http://localhost:11434"
model = "qwen2.5-coder:14b"
role = "sub"
weight = 1

# Remote Ollama on big72 (optional additional capacity)
# [[providers]]
# provider_type = "ollama"
# base_url = "http://big72.local:11434"
# model = "qwen2.5-coder:14b"
# role = "sub"
# weight = 1
