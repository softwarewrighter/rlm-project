# RLM Orchestrator Configuration - LiteLLM Proxy
#
# Use this config to route through LiteLLM for usage tracking.
# Requires LITELLM_API_KEY or LITELLM_MASTER_KEY env var.
#
# Usage:
#   export LITELLM_API_KEY=sk-...
#   ./target/release/rlm-server config-litellm.toml

max_iterations = 20
max_sub_calls = 50
output_limit = 10000

# Smart bypass: skip RLM for small contexts
bypass_enabled = true
bypass_threshold = 4000

# Level priority order
level_priority = ["dsl", "wasm", "cli", "llm_delegation"]

# Level 1: DSL (enabled by default)
[dsl]
enabled = true

# Level 2: WASM (enabled by default)
[wasm]
enabled = true
rust_wasm_enabled = true
fuel_limit = 1000000
memory_limit = 67108864
cache_size = 100
codegen_provider = "litellm"
codegen_url = "http://localhost:4000"
codegen_model = "deepseek/deepseek-coder"

# Level 3: CLI (disabled by default)
[cli]
enabled = false

# Level 4: LLM Delegation (disabled by default)
[llm_delegation]
enabled = false

# LiteLLM Proxy for root LLM (usage tracked via LiteLLM dashboard)
[[providers]]
provider_type = "litellm"
base_url = "http://localhost:4000"
model = "deepseek-chat"
role = "root"
weight = 1

# Local Ollama for sub-LM calls (free, no tracking needed)
[[providers]]
provider_type = "ollama"
base_url = "http://localhost:11434"
model = "gemma2:9b"
role = "sub"
weight = 1
