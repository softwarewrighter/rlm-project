# RLM Orchestrator Configuration - LiteLLM with CLI Enabled
#
# Uses DeepSeek models via LiteLLM for both root LLM and code generation.
# CLI mode enabled for L3 demos.
#
# Usage:
#   export LITELLM_API_KEY=sk-...
#   ./target/release/rlm-server config-litellm-cli.toml

max_iterations = 20
max_sub_calls = 50
output_limit = 10000

# Smart bypass: skip RLM for small contexts
bypass_enabled = true
bypass_threshold = 4000

# Level priority order
level_priority = ["dsl", "wasm", "cli", "llm_delegation"]

# Level 1: DSL (enabled by default)
[dsl]
enabled = true

# Level 2: WASM (enabled)
[wasm]
enabled = true
rust_wasm_enabled = true
fuel_limit = 1000000
memory_limit = 67108864
cache_size = 100
codegen_provider = "litellm"
codegen_url = "http://localhost:4000"
codegen_model = "deepseek-coder"

# Level 3: CLI (ENABLED for L3 demos)
[cli]
enabled = true
sandbox_mode = "none"
timeout_secs = 30
max_output_size = 10485760

[cli.validation]
allow_filesystem_read = false
allow_filesystem_write = false
allow_network = false
allow_process_spawn = false
allow_unsafe = false

# Level 4: LLM Delegation (disabled by default)
[llm_delegation]
enabled = false

# Root LLM via LiteLLM - DeepSeek Chat
[[providers]]
provider_type = "litellm"
base_url = "http://localhost:4000"
model = "deepseek-coder"
role = "both"
weight = 1
