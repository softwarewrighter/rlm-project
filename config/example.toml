# RLM Orchestrator Configuration
# Copy to config/local.toml and customize for your setup

[server]
host = "0.0.0.0"
port = 8080
grpc_port = 50051

[runtime]
# Tokio runtime - adjust based on your CPU
# For dual Xeon with 72 threads: worker_threads = 60
worker_threads = 60
blocking_threads = 12
thread_stack_size = 8388608  # 8MB

[parallelism]
# Sub-LM dispatch configuration
max_parallel_sub_calls = 20
batch_size = 4
use_speculation = true
speculation_threads = 4

[caching]
# Cache configuration
enable_semantic_cache = true
semantic_threshold = 0.85
max_cache_mb = 4096          # 4GB - adjust based on available RAM
embedding_model = "nomic-embed-text"

[memory]
# Memory configuration for large contexts
use_mmap = true
max_context_mb = 10240       # 10GB max context
chunk_size = 10000           # 10KB default chunks

[pipeline]
# Pipeline parallelism
enable_pipelining = true
lookahead_depth = 2
prefetch_chunks = 5

[limits]
# Safety limits
max_iterations = 30
max_sub_calls_per_iteration = 50
session_timeout_minutes = 60

[repl]
# Python REPL configuration
timeout_seconds = 300
max_output_chars = 50000
sandbox_enabled = true

# =============================================================================
# LLM Providers
# =============================================================================

[providers]

# -----------------------------------------------------------------------------
# Ollama Servers (Local/LAN)
# Priority: Lower number = preferred
# -----------------------------------------------------------------------------

[[providers.ollama]]
name = "m40-primary"
host = "192.168.1.10"        # Change to your server IP
port = 11434
models = ["qwen2.5-coder:32b", "llama3.3:70b"]
priority = 1
max_concurrent = 2
gpu_memory_gb = 24.0

[[providers.ollama]]
name = "rtx-secondary"
host = "192.168.1.11"        # Change to your server IP
port = 11434
models = ["llama3.3:70b", "deepseek-coder:33b"]
priority = 2
max_concurrent = 1
gpu_memory_gb = 16.0

[[providers.ollama]]
name = "p100-batch"
host = "192.168.1.12"        # Change to your server IP
port = 11434
models = ["qwen2.5-coder:14b", "llama3.2:3b"]
priority = 3
max_concurrent = 4
gpu_memory_gb = 32.0         # 2x P100 16GB

# For single local server, use just:
# [[providers.ollama]]
# name = "local"
# host = "localhost"
# port = 11434
# models = ["qwen2.5-coder:32b"]
# priority = 1
# max_concurrent = 1
# gpu_memory_gb = 24.0

# -----------------------------------------------------------------------------
# DeepSeek API (Cloud)
# -----------------------------------------------------------------------------

[providers.deepseek]
api_key_env = "DEEPSEEK_API_KEY"   # Set this environment variable
base_url = "https://api.deepseek.com"

[providers.deepseek.models]
root = "deepseek-chat"             # For root LLM queries
sub = "deepseek-chat"              # For sub-LLM queries (could use cheaper model)

# -----------------------------------------------------------------------------
# Claude API (Cloud)
# -----------------------------------------------------------------------------

[providers.claude]
api_key_env = "ANTHROPIC_API_KEY"  # Set this environment variable

[providers.claude.models]
root = "claude-sonnet-4-20250514"
sub = "claude-haiku-4-5-20251001"  # Cheaper for sub-queries

# -----------------------------------------------------------------------------
# OpenAI-Compatible (llama.cpp server, vLLM, etc.)
# -----------------------------------------------------------------------------

[[providers.openai_compatible]]
name = "llama-cpp-local"
base_url = "http://localhost:8080"
api_key_env = ""                   # Optional, leave empty if not needed
models = ["local-model"]

# =============================================================================
# Provider Selection
# =============================================================================

[provider_selection]
# Which provider to use by default
# Options: "ollama", "deepseek", "claude", "openai_compatible"
root_provider = "ollama"
sub_provider = "ollama"

# Load balancing strategy for Ollama pool
# Options: "round_robin", "least_loaded", "priority", "adaptive"
load_balance_strategy = "adaptive"

# =============================================================================
# Dogfooding Configuration
# =============================================================================

[dogfooding]
enabled = false
auto_approve = false              # Require human approval for changes
max_suggestions_per_cycle = 10
test_coverage_threshold = 0.8
codebase_paths = ["src/"]

# =============================================================================
# Telemetry
# =============================================================================

[telemetry]
enable_metrics = true
metrics_port = 9090               # Prometheus metrics endpoint
enable_tracing = false
tracing_endpoint = ""             # Jaeger/OTLP endpoint

# =============================================================================
# Logging
# =============================================================================

[logging]
level = "info"                    # trace, debug, info, warn, error
format = "pretty"                 # pretty, json, compact
file = ""                         # Optional log file path
