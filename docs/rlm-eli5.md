# Recursive Language Models (RLM) - Explain Like I'm 5

## The Cookie Jar Problem ğŸª

Imagine you have a **really, really big cookie jar** - so big you can't see all the cookies at once. You want to find all the chocolate chip cookies.

**Normal way (regular LLM):** Try to dump ALL the cookies on the table at once. But your table is too small! Cookies fall off, you lose track, and you miss some chocolate chips.

**Smart way (RLM):** 
1. Look at a handful of cookies at a time
2. Ask your friend to check each handful: "Any chocolate chips here?"
3. Keep track of what your friend finds
4. When done, add up all the chocolate chips!

That's RLM! Instead of forcing everything into the AI's brain at once (where it gets confused), we let the AI **look at pieces** and **ask helper AIs** about each piece.

---

## The Three Magic Powers of RLM

### 1. ğŸ“¦ The Context Box
Instead of eating all the text, the AI puts it in a box and looks at it piece by piece.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONTEXT BOX                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Doc 1â”‚ â”‚Doc 2â”‚ â”‚Doc 3â”‚ â”‚ ... â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  "I can peek at any piece I want!"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. ğŸ’» The Code Superpower
The AI can write Python code to search, filter, and organize the text - like having a robot assistant!

```python
# AI writes this code itself!
for doc in documents:
    if "chocolate" in doc:
        interesting_docs.append(doc)
```

### 3. ğŸ¤– The Helper AI Phone
The main AI can call helper AIs to analyze each piece. Like having friends who can each read one chapter of a huge book!

```
Main AI: "Hey helper, what's in chapter 5?"
Helper AI: "It talks about dragons and a magic sword!"
Main AI: "Thanks! *writes that down* Now checking chapter 6..."
```

---

## Real Example: Finding Needles in Haystacks

**Task:** Find who won the beauty pageant in a 10-million-word document collection.

### Without RLM (Regular AI):
```
AI: *tries to read 10 million words*
AI: *brain melts* 
AI: "Uhh... I think maybe... Susan? No wait... I forgot..."
```

### With RLM:
```
Step 1: AI writes code to list all documents
        "Okay, I have 1000 documents totaling 10M words"

Step 2: AI searches for keywords
        documents = grep("beauty pageant", all_docs)
        "Found 5 documents mentioning beauty pageant!"

Step 3: AI asks helper to check each one
        for doc in documents:
            answer = helper_ai("Who won the pageant?", doc)
            results.append(answer)
        
Step 4: AI combines results
        "Based on 3 matching answers: Maria Dalmacio won!"
```

---

## Mike's Implementation Options

Here are the ways you could build RLM with your setup:

### Option A: Custom Rust Orchestrator ğŸ¦€

**What it is:** A Rust program that coordinates everything - loads documents, runs a Python REPL, and calls your LLMs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RUST RLM ORCHESTRATOR                    â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Context  â”‚    â”‚  Python  â”‚    â”‚    LLM Pool      â”‚  â”‚
â”‚  â”‚  Store   â”‚â—„â”€â”€â–ºâ”‚   REPL   â”‚â—„â”€â”€â–ºâ”‚                  â”‚  â”‚
â”‚  â”‚(HashMap) â”‚    â”‚  (PyO3)  â”‚    â”‚ Ollama (M40s)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Ollama (RTX)     â”‚  â”‚
â”‚                                   â”‚ Ollama (P100s)   â”‚  â”‚
â”‚                                   â”‚ DeepSeek API     â”‚  â”‚
â”‚                                   â”‚ Claude API       â”‚  â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Pros | Cons |
|------|------|
| âœ… Full control over everything | âŒ Most development work |
| âœ… Optimal for your GPU cluster | âŒ Need to maintain it yourself |
| âœ… Can load-balance across servers | âŒ ~2-4 weeks to build properly |
| âœ… Native performance | âŒ Python REPL integration adds complexity |
| âœ… Your preferred language! | |

**Best for:** Production use, processing huge documents regularly, when you need to squeeze every bit of performance from your hardware.

---

### Option B: OpenCode + DeepSeek API ğŸ”·

**What it is:** Use Z.ai's opencode CLI with DeepSeek as the backend, wrapped with RLM capabilities.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPENCODE + WRAPPER                    â”‚
â”‚                                                          â”‚
â”‚   User Query                                             â”‚
â”‚       â”‚                                                  â”‚
â”‚       â–¼                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”‚  rlm-wrapper.sh â”‚  â—„â”€â”€ Injects RLM system prompt    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚            â”‚                                             â”‚
â”‚            â–¼                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚    opencode     â”‚â”€â”€â”€â–ºâ”‚   DeepSeek API   â”‚          â”‚
â”‚   â”‚  (code executor)â”‚    â”‚  (deepseek-chat) â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚            â”‚                                             â”‚
â”‚            â–¼                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”‚  Python REPL    â”‚  â—„â”€â”€ llm_query() calls back to   â”‚
â”‚   â”‚  + llm_query()  â”‚      DeepSeek or local Ollama    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Pros | Cons |
|------|------|
| âœ… Fast to set up (~1 day) | âŒ DeepSeek API costs money |
| âœ… DeepSeek is very capable | âŒ Less control than custom solution |
| âœ… OpenCode handles code execution | âŒ Depends on external API availability |
| âœ… Can fall back to local Ollama | âŒ OpenCode still maturing |
| âœ… GLM-4 support too | |

**Best for:** Quick experiments, when you want good results fast, hybrid cloud/local setup.

---

### Option C: Pure Ollama + Python Script ğŸ¦™

**What it is:** A standalone Python script that implements RLM using only your local Ollama servers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PYTHON RLM SCRIPT                       â”‚
â”‚                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                  rlm.py                          â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚  context_store = {"context": big_document}      â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚  while not done:                                 â”‚   â”‚
â”‚   â”‚      code = ask_root_llm("what next?")          â”‚   â”‚
â”‚   â”‚      output = exec(code)  # runs in REPL        â”‚   â”‚
â”‚   â”‚      if "FINAL" in output:                       â”‚   â”‚
â”‚   â”‚          done = True                             â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                               â”‚
â”‚                          â–¼                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              YOUR OLLAMA SERVERS                 â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚  Server 1 (M40 24GB)     Server 2 (RTX)         â”‚   â”‚
â”‚   â”‚  â””â”€ qwen2.5-coder:32b    â””â”€ llama3.3:70b        â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚  Server 3 (P100s)                                â”‚   â”‚
â”‚   â”‚  â””â”€ deepseek-coder:33b                           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Pros | Cons |
|------|------|
| âœ… Completely free (your hardware) | âŒ Slower than cloud APIs |
| âœ… Works offline | âŒ Limited by your GPU VRAM |
| âœ… Simple to understand & modify | âŒ No fancy load balancing built-in |
| âœ… Great for learning/experimenting | âŒ Python, not Rust ğŸ˜‰ |
| âœ… $0.45/kWh + solar = very cheap | |

**Best for:** Privacy-sensitive work, learning how RLM works, when internet is unreliable, cost optimization.

---

### Option D: Claude Code CLI + MCP Server ğŸ”Œ

**What it is:** Extend Claude Code with custom MCP tools that provide RLM capabilities.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                CLAUDE CODE + MCP RLM                     â”‚
â”‚                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”‚   Claude Code   â”‚                                   â”‚
â”‚   â”‚   CLI / Web     â”‚                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚            â”‚ MCP Protocol                                â”‚
â”‚            â–¼                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚            MCP RLM Server                        â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚  Tools:                                          â”‚   â”‚
â”‚   â”‚  â€¢ load_context(name, content)                  â”‚   â”‚
â”‚   â”‚  â€¢ peek_context(name, start, end)               â”‚   â”‚
â”‚   â”‚  â€¢ context_info(name)                           â”‚   â”‚
â”‚   â”‚  â€¢ llm_subquery(prompt, provider, model)        â”‚   â”‚
â”‚   â”‚  â€¢ execute_code(code)                           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                               â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚            â–¼                           â–¼                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚  Ollama Servers â”‚       â”‚   Claude API    â”‚        â”‚
â”‚   â”‚  (sub-queries)  â”‚       â”‚  (sub-queries)  â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Pros | Cons |
|------|------|
| âœ… Integrates with existing Claude workflow | âŒ Depends on Claude Code availability |
| âœ… MCP is extensible standard | âŒ Two AI layers (Claude + sub-LLM) |
| âœ… Can use Claude's strong reasoning | âŒ Costs money (Claude API) |
| âœ… Easy to add more tools later | âŒ MCP server needs to stay running |
| âœ… Hybrid local/cloud naturally | |

**Best for:** When you're already using Claude Code, want best-of-both-worlds, professional work.

---

### Option E: Hybrid Rust + Emacs Integration ğŸš€

**What it is:** Rust daemon with elisp bindings for Emacs integration.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                EMACS + RUST RLM DAEMON                   â”‚
â”‚                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                    EMACS                         â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚  (rlm-query "Find all TODO items"               â”‚   â”‚
â”‚   â”‚             (buffer-string))                     â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚  ;; Communicates via JSON-RPC or socket         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                               â”‚
â”‚                          â–¼                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              RUST RLM DAEMON                     â”‚   â”‚
â”‚   â”‚                                                  â”‚   â”‚
â”‚   â”‚  â€¢ Runs as background service                   â”‚   â”‚
â”‚   â”‚  â€¢ Manages context across sessions              â”‚   â”‚
â”‚   â”‚  â€¢ Load balances across GPU servers             â”‚   â”‚
â”‚   â”‚  â€¢ Caches frequent queries                      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                               â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚            â–¼                           â–¼                â”‚
â”‚      Local Ollama              Cloud APIs               â”‚
â”‚      (your GPUs)           (fallback/overflow)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Pros | Cons |
|------|------|
| âœ… Native Emacs integration | âŒ Most complex to build |
| âœ… Persistent daemon = fast startup | âŒ Need elisp + Rust expertise |
| âœ… Perfect for your workflow | âŒ 4-6 weeks development |
| âœ… Can integrate with org-mode, magit | âŒ Niche (just for you!) |
| âœ… Full Rust performance | |

**Best for:** Ultimate integration with your Emacs workflow, long-term investment.

---

## Quick Comparison Chart

| Option | Setup Time | Cost | Performance | Control | Your Match |
|--------|------------|------|-------------|---------|------------|
| A. Rust Custom | 2-4 weeks | $0* | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| B. OpenCode+DeepSeek | 1 day | $$ | â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| C. Python+Ollama | 2-3 days | $0* | â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| D. Claude+MCP | 1-2 days | $$$ | â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| E. Emacs+Rust | 4-6 weeks | $0* | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |

*$0 = just electricity for your servers

---

## The Secret Sauce: Why RLM Works

### Traditional LLM: "Context Rot" ğŸ¤¢
```
Input size:    [============================] 10M tokens
Model window:  [======]                        272K tokens
Result:        ğŸ¤¯ "I... forgot... what was the question?"
```

### RLM: "Divide and Conquer" ğŸ’ª
```
Input size:    [============================] 10M tokens
                  â†“ chunk â†“ chunk â†“ chunk
Sub-queries:   [==] â†’ answer1
               [==] â†’ answer2  
               [==] â†’ answer3
                  â†“ combine
Final:         ğŸ¯ Accurate answer from all pieces!
```

The paper shows RLM achieves:
- **91.33%** accuracy on BrowseComp+ (vs 0% for base model that couldn't fit context!)
- **58%** F1 on OOLONG-Pairs (vs 0.04% for GPT-5 base)
- Handles **10M+ tokens** effectively

---

## Getting Started: Recommended Path

Based on your setup (Arch Linux, distributed GPUs, Rust preference, Emacs user):

### Week 1: Quick Win
Start with **Option C (Python + Ollama)** to understand how RLM works.

### Week 2-3: Production Path
Build **Option A (Rust Orchestrator)** with your learnings.

### Week 4+: Integration
Add **Option E (Emacs bindings)** for daily workflow integration.

### Parallel Track
Set up **Option D (MCP Server)** for when you're using Claude Code anyway.

---

## One More Analogy: The Library Research Assistant ğŸ“š

**You:** "Find everything about quantum computing in this library."

**Regular AI (tries to read entire library):**
*head explodes* ğŸ“šğŸ’¥ğŸ¤¯

**RLM AI:**
1. "Let me check the card catalog first..." *(probes structure)*
2. "Physics section, rows 12-15 look relevant..." *(filters)*
3. "Hey assistant, summarize book 12A" *(sub-query)*
4. "Hey assistant, summarize book 12B" *(sub-query)*
5. "Hey assistant, summarize book 12C" *(sub-query)*
6. "Combining all summaries... here's your answer!" *(aggregate)*

**Result:** Accurate, comprehensive, and didn't need to read every cookbook in the library! ğŸ‰

---

## TL;DR

1. **RLM = Let AI peek at big data in pieces + call helper AIs**
2. **Your best options:** Rust orchestrator (production) or Python+Ollama (learning)
3. **Why it works:** Avoids "context rot" by dividing and conquering
4. **Key insight:** The prompt is a variable, not input - the AI manipulates it with code

Now go build something cool! ğŸš€
